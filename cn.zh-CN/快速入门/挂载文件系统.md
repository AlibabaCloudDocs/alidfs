# 挂载文件系统 {#task_vmg_jtk_z2b .task}

创建文件系统并添加挂载点后，您可以通过挂载点挂载分布式文件系统实例。

对于 ECS 实例来说，能否通过一个挂载点访问文件系统取决于以下的条件：

-   若挂载点网络类型是专有网络，则只有同一 VPC 内的 ECS 实例能够挂载，并且挂载点所绑定的权限组中有一条规则的授权地址能够与 ECS 实例的 VPC IP 地址匹配。
-   HDFS 协议挂载前，您需要确保 ECS 中已经安装了 java 1.8。

1.  在一个节点上将以下内容添加到core-site.xml文件，并同步到所有依赖 hadoop-common 的节点上： 

    ```
    <property>
         <name>fs.defaultFS</name>
         <value>dfs://DfsInstanceID.RegionID.alidfs.aliyun.com:10290</value>
    </property>
    <property>
         <name>fs.dfs.impl</name>
         <value>com.alibaba.dfs.DistributedFileSystem</value>
    </property>
    <property>
         <name>fs.AbstractFileSystem.dfs.impl</name>
         <value>com.alibaba.dfs.DFS</value>
    </property>
    
    ```

    **说明：** 

    -   请将 RegionID 和 DfsInstanceID 替换为具体 DFS 实例的相应内容。
    -   core-site.xml的内容需要同步到所有依赖 hadoop-common 的节点上。
    -   DFS 路径 URI 格式为：dfs://DfsInstanceID.RegionID.alidfs.aliyun.com:10290，例如：dfs://f-63a47d43wh98.cn-neimeng-env10-d01.alidfs.aliyun.com:10290
2.  下载aliyun-sdk-dfs-x.y.z.jar，将其部署在 Hadoop 生态系统组件的 CLASSPATH 上，推荐将其部署到hadoop-common-X.YZ.jar所在的目录内。 

    例如，对于 Spark 2.3.0，解压后的目录结构如下图：

    ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/19052/154337121411125_zh-CN.png)

    需要把aliyun-sdk-dfs-x.y.z.jar拷贝到jars目录。

3.  在core-site.xml中配置参数（如io.file.buffer.size和dfs.connection.count等），并同步到所有依赖 hadoop-common 的节点上。 

    ```
    <property>
         <name>io.file.buffer.size</name>
         <value>4194304</value>
         <description>To achieve high throughput, no less than 1MB, no more than 8MB</description>
    </property>
    <property>
         <name>dfs.connection.count</name> 
         <value>1</value>
         <description>If multi threads in the same process will read/write to DFS, set to count of threads</description>
    </property>
    ```

4.  使用 hadoop fs 命令行工具，运行`hadoop fs -ls /`命令进行验证，如下图所示：![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/19052/154337121411130_zh-CN.png)

 如未报错，则部署成功。

