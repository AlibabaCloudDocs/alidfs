# 挂载文件系统 {#task_vmg_jtk_z2b .task}

创建文件系统并添加挂载点后，您可以通过挂载点挂载文件存储HDFS实例。

对于ECS实例来说，能否通过一个挂载点访问文件系统取决于以下的条件：

-   若挂载点网络类型是专有网络，则只有同一VPC内的ECS实例能够挂载，并且挂载点所绑定的权限组中有一条规则的授权地址能够与ECS实例的VPC IP地址匹配。
-   HDFS协议挂载前，您需要确保ECS中已经安装了java 1.8。

1.  在一个节点上将以下内容添加到core-site.xml文件，并同步到所有依赖hadoop-common的节点上： 

    ```
    <property>
         <name>fs.defaultFS</name>
         <value>dfs://DfsMountpointDomainName:10290</value>
    </property>
    <property>
         <name>fs.dfs.impl</name>
         <value>com.alibaba.dfs.DistributedFileSystem</value>
    </property>
    <property>
         <name>fs.AbstractFileSystem.dfs.impl</name>
         <value>com.alibaba.dfs.DFS</value>
    </property>
    
    ```

    **说明：** 

    -   请将MountpointDomainName替换为具体文件存储HDFS实例的挂载地址，如xxx.cn-hangzhou.dfs.aliyuncs.com。

        ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/19052/155409752842101_zh-CN.jpg)

    -   core-site.xml的内容需要同步到所有依赖hadoop-common的节点上。
2.  下载aliyun-sdk-dfs-x.y.z.jar，将其部署在Hadoop生态系统组件的CLASSPATH 上，推荐将其部署到hadoop-common-X.YZ.jar所在的目录内。 

    **说明：** Hadoop版本不低于2.7.2。

    例如，对于Spark 2.3.0，解压后的目录结构如下图：

    ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/19052/155409752811125_zh-CN.png)

    需要把aliyun-sdk-dfs-x.y.z.jar拷贝到jars目录。

3.  在core-site.xml中配置参数（如io.file.buffer.size和dfs.connection.count等），并同步到所有依赖hadoop-common的节点上。 

    ```
    <property>
         <name>io.file.buffer.size</name>
         <value>4194304</value>
         <description>To achieve high throughput, no less than 1MB, no more than 8MB</description>
    </property>
    <property>
         <name>dfs.connection.count</name> 
         <value>1</value>
         <description>If multi threads in the same process will read/write to DFS, set to count of threads</description>
    </property>
    ```

4.  使用hadoop fs命令行工具，运行`hadoop fs -ls /`命令进行验证，如下图所示：![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/19052/155409752811130_zh-CN.png)

 如未报错，则部署成功。

