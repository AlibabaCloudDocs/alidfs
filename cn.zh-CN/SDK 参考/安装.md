# 安装 {#concept_bmh_bdf_qfb .concept}

本文档主要介绍文件系统SDK的安装及使用方式。

本节以hadoop-mapreduce-examples为例，介绍文件系统SDK的使用方式。其中MapReduce以伪分布式方式运行。有关MapReduce的伪分布方式，请参见[Apache Hadoop](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation) 文档说明。

## 环境准备 {#section_t4s_ndf_qfb .section}

1.  运行java ‐version查看JDK版本。

    JDK版本不能低于 1.8。

2.  安装完成后，按照以下方式设置环境变量，假设安装位置为/opt/install/java。

    ```
    JAVA_HOME=/opt/install/java
    PATH=/opt/install/java/bin:$PATH
    ```


## 下载 SDK {#section_chx_xdf_qfb .section}

您可以单击[此处](https://mvnrepository.com/artifact/com.aliyun.dfs/aliyun-sdk-dfs)下载文件存储HDFS文件系统SDK的JAR文件aliyun-sdk-dfs-x.y.z.jar。

## 配置 Hadoop {#section_gxc_c2f_qfb .section}

1.  下载hadoop 2.7.2发布包。
2.  运行tar ‐zxvf hadoop‐2.7.2.tar.gz命令，解压缩下载的发布包。
3.  运行export HADOOP\_HOME=yourWorkingDir/hadoop‐2.7.2命令，设置环境变量。
4.  运行cd hadoop-2.7.2命令，进入 Hadoop 目录。
5.  修改etc/hadoop/hadoop-env.sh文件，并增加环境准备中设置的`JAVA_HOME`：

    ```
    # set to the root of your Java installation
    export JAVA_HOME=youJAVADirt
    ```

6.  修改etc/hadoop/core-site.xml文件，core-site.xml 文件中需要修改的内容如下所示：

    ```
    <property>
    <name>fs.defaultFS</name>
    <value>dfs://DfsMountpointDomainName:10290</value>
    </property>
    <property>
    <name>fs.dfs.impl</name>
    <value>com.alibaba.dfs.DistributedFileSystem</value>
    </property>
    <property>
    <name>fs.AbstractFileSystem.dfs.impl</name>
    <value>com.alibaba.dfs.DFS</value>
    </property>
    						
    ```

    **说明：** 

    -   请将MountpointDomainName替换为具体文件存储HDFS实例的挂载地址，如xxx.cn-hangzhou.dfs.aliyuncs.com。

        ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/40523/155747787542080_zh-CN.jpg)

    -   core-site.xml的内容需要同步到所有依赖`hadoop-common`的节点上。

## 部署依赖 {#section_ikk_zff_qfb .section}

将上述步骤中获得的aliyun-sdk-dfs-x.y.z.jar 拷贝至Hadoop生态系统组件的CLASSPATH 上。推荐将其部署到hadoop-common-x.y.z.jar所在的目录内，并复制到所有Hadoop节点。对于MapReduce组件，该目录为$HADOOP\_HOME/share/hadoop/hdfs。

## 验证安装 {#section_ymj_fgf_qfb .section}

请执行以下步骤验证安装：

1.  准备数据
    1.  运行以下命令创建目录：

        ```
        $HADOOP_HOME/bin/hadoop fs ‐mkdir inputDir
        ```

    2.  运行以下命令上传文件：

        ```
        touch a.txt
        $HADOOP_HOME/bin/hadoop fs ‐put a.txt inputDir/
        ```

2.  重启yarn服务

    运行以下命令重启yarn服务：

    ``` {#codeblock_ad2_ubg_bwf}
    $HADOOP_HOME/sbin/stop-yarn.sh
    $HADOOP_HOME/sbin/start-yarn.sh
    ```

3.  执行样例测试

    在$HADOOP\_HOME下执行以下样例：

    -   wordcount样例

        ```
        bin/hadoop jar ./share/hadoop/mapreduce/hadoop‐mapreduce‐examples‐2.7.2.jar wordcount
        inputDir outputDir
        ```

    -   grep样例

        ```
        bin/hadoop jar ./share/hadoop/mapreduce/hadoop‐mapreduce‐examples‐2.7.2.jar grep
        inputDir/ outputDirGrep/ "the"
        ```


