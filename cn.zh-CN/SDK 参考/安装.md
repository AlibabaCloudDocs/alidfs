# 安装 {#concept_bmh_bdf_qfb .task}

本文档主要介绍文件系统SDK的安装及使用方式。

## 环境准备 {#section_t4s_ndf_qfb .section}

本节以hadoop-mapreduce-examples为例，介绍文件系统SDK的使用方式。其中MapReduce以伪分布式方式运行。有关MapReduce的伪分布方式，请参见[Apache Hadoop](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation)文档说明。

1.  运行`java -version`命令，查看JDK版本。 

    JDK版本不能低于1.8。

2.  按照以下方式设置环境变量，假设安装位置为/opt/install/java。 

    ``` {#codeblock_ri5_cbf_oyh}
    JAVA_HOME=/opt/install/java
    PATH=/opt/install/java/bin:$PATH
    ```


## 下载 SDK {#section_chx_xdf_qfb .section}

您可以单击[此处](https://mvnrepository.com/artifact/com.aliyun.dfs/aliyun-sdk-dfs)下载文件存储HDFS文件系统SDK的JAR文件aliyun-sdk-dfs-x.y.z.jar。

## 配置 Hadoop {#section_gxc_c2f_qfb .section}

1.  下载hadoop 2.7.2发布包。
2.  运行`tar -zxvf hadoop-2.7.2.tar.gz`命令，解压缩下载的发布包。
3.  运行`export HADOOP_HOME=yourWorkingDir/hadoop-2.7.2`命令，设置环境变量。
4.  运行`cd hadoop-2.7.2`命令，进入Hadoop目录。
5.  修改etc/hadoop/hadoop-env.sh文件，并增加环境准备中设置的`JAVA_HOME`。 

    ``` {#codeblock_m7e_j9k_k7o}
    # set to the root of your Java installation
    export JAVA_HOME=youJAVADirt
    ```

6.  修改etc/hadoop/core-site.xml文件，core-site.xml文件中需要修改的内容如下所示。 

    ``` {#codeblock_59t_66n_wh0}
    <property>
    <name>fs.defaultFS</name>
    <value>dfs://DfsMountpointDomainName:10290</value>
    </property>
    <property>
    <name>fs.dfs.impl</name>
    <value>com.alibaba.dfs.DistributedFileSystem</value>
    </property>
    <property>
    <name>fs.AbstractFileSystem.dfs.impl</name>
    <value>com.alibaba.dfs.DFS</value>
    </property>
    						
    ```

    **说明：** 

    -   请将MountpointDomainName替换为具体文件存储HDFS实例的挂载地址，如xxx.cn-hangzhou.dfs.aliyuncs.com。

        ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/40523/156637583142080_zh-CN.jpg)

    -   core-site.xml的内容需要同步到所有依赖`hadoop-common`的节点上。

## 部署依赖 {#section_ikk_zff_qfb .section}

将上述步骤中获得的aliyun-sdk-dfs-x.y.z.jar拷贝至Hadoop生态系统组件的CLASSPATH 上。推荐将其部署到hadoop-common-x.y.z.jar所在的目录内，并复制到所有Hadoop节点。对于MapReduce组件，该目录为$HADOOP\_HOME/share/hadoop/hdfs。

## 验证安装 {#section_ymj_fgf_qfb .section}

请执行以下步骤验证安装。

1.  准备数据。 
    1.  运行以下命令创建目录。 

        ``` {#codeblock_tyd_sqe_cur}
        $HADOOP_HOME/bin/hadoop fs -mkdir -p inputDir
        ```

    2.  运行以下命令上传文件。 

        ``` {#codeblock_cr6_l7k_rb2}
        touch a.txt
        $HADOOP_HOME/bin/hadoop fs -put a.txt inputDir/
        ```

2.  重启yarn服务。 

    运行以下命令重启yarn服务。

    ``` {#codeblock_tnm_8bb_1sf}
    $HADOOP_HOME/sbin/stop-yarn.sh
    $HADOOP_HOME/sbin/start-yarn.sh
    ```

3.  执行样例测试。 

    在$HADOOP\_HOME下执行以下样例。

    -   wordcount样例

        ``` {#codeblock_l8m_6gh_dzx}
        bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount
        inputDir outputDir
        ```

    -   grep样例

        ``` {#codeblock_anc_6n0_7bu}
        bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep
        inputDir/ outputDirGrep/ "the"
        ```


